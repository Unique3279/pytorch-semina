{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a008198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 7.6522, Weight: 1.2607\n",
      "Epoch 10, Loss: 0.0182, Weight: 1.9639\n",
      "Epoch 20, Loss: 0.0000, Weight: 1.9982\n",
      "Epoch 30, Loss: 0.0000, Weight: 1.9999\n",
      "Epoch 40, Loss: 0.0000, Weight: 2.0000\n",
      "Epoch 50, Loss: 0.0000, Weight: 2.0000\n",
      "Epoch 60, Loss: 0.0000, Weight: 2.0000\n",
      "Epoch 70, Loss: 0.0000, Weight: 2.0000\n",
      "Epoch 80, Loss: 0.0000, Weight: 2.0000\n",
      "Epoch 90, Loss: 0.0000, Weight: 2.0000\n",
      "Final Weight: 2.0000 Elapsed Time: 0.00 ms\n",
      "Predictions: x = 4.0 -> 7.9999999999996945\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "w = 1.0\n",
    "\n",
    "def foward(x):\n",
    "    return w * x\n",
    "\n",
    "def loss(x, y):\n",
    "    y_pred = foward(x)\n",
    "    return (y_pred - y) ** 2\n",
    "\n",
    "def gradient(x, y):\n",
    "    return 2 * x * (foward(x) - y)\n",
    "\n",
    "# train loop\n",
    "train_time = time.time()\n",
    "for epoch in range(100):\n",
    "    for x, y in zip(x_data, y_data):\n",
    "        grad = gradient(x, y)\n",
    "        w -= 0.01 * grad  # update weight with learning rate of 0.01\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        total_loss = sum(loss(x, y) for x, y in zip(x_data, y_data))\n",
    "        print(f'Epoch {epoch}, Loss: {total_loss:.4f}, Weight: {w:.4f}')\n",
    "finish_time = time.time()\n",
    "\n",
    "# Final output\n",
    "print(f'Final Weight: {w:.4f} Elapsed Time: {(finish_time - train_time) * 1000.0:.2f} ms')\n",
    "print(f\"Predictions: x = 4.0 -> {foward(4.0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e122ec",
   "metadata": {},
   "source": [
    "##### Exercise 3-1\n",
    "\n",
    "$$\\hat{y} = w_2 x^2 + w_1 x + b$$\n",
    "$$loss = (\\hat{y} - y)^2$$\n",
    "\n",
    "$$\\frac{\\partial loss}{\\partial w_1} = 2x \\left(xw_{1} - y + w_{2} x^{2} + b\\right)$$\n",
    "$$\\frac{\\partial loss}{\\partial w_2} = 2x^{2} \\left(x^{2} w_{2} - y + w_{1} x + b\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e6c98a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7838, Weights: w_2=0.3108, w_1=0.0814\n",
      "Epoch 10, Loss: 0.7745, Weights: w_2=0.2746, w_1=0.1045\n",
      "Epoch 20, Loss: 0.6668, Weights: w_2=0.2692, w_1=0.1330\n",
      "Epoch 30, Loss: 0.5740, Weights: w_2=0.2642, w_1=0.1595\n",
      "Epoch 40, Loss: 0.4942, Weights: w_2=0.2596, w_1=0.1841\n",
      "Epoch 50, Loss: 0.4255, Weights: w_2=0.2553, w_1=0.2069\n",
      "Epoch 60, Loss: 0.3663, Weights: w_2=0.2513, w_1=0.2280\n",
      "Epoch 70, Loss: 0.3154, Weights: w_2=0.2476, w_1=0.2476\n",
      "Epoch 80, Loss: 0.2715, Weights: w_2=0.2442, w_1=0.2658\n",
      "Epoch 90, Loss: 0.2338, Weights: w_2=0.2410, w_1=0.2827\n",
      "Epoch 100, Loss: 0.2013, Weights: w_2=0.2380, w_1=0.2984\n",
      "Epoch 110, Loss: 0.1733, Weights: w_2=0.2353, w_1=0.3129\n",
      "Epoch 120, Loss: 0.1492, Weights: w_2=0.2327, w_1=0.3264\n",
      "Epoch 130, Loss: 0.1284, Weights: w_2=0.2304, w_1=0.3389\n",
      "Epoch 140, Loss: 0.1106, Weights: w_2=0.2282, w_1=0.3506\n",
      "Epoch 150, Loss: 0.0952, Weights: w_2=0.2262, w_1=0.3613\n",
      "Epoch 160, Loss: 0.0820, Weights: w_2=0.2243, w_1=0.3713\n",
      "Epoch 170, Loss: 0.0706, Weights: w_2=0.2225, w_1=0.3806\n",
      "Epoch 180, Loss: 0.0607, Weights: w_2=0.2209, w_1=0.3892\n",
      "Epoch 190, Loss: 0.0523, Weights: w_2=0.2194, w_1=0.3972\n",
      "Epoch 200, Loss: 0.0450, Weights: w_2=0.2180, w_1=0.4046\n",
      "Epoch 210, Loss: 0.0388, Weights: w_2=0.2167, w_1=0.4115\n",
      "Epoch 220, Loss: 0.0334, Weights: w_2=0.2155, w_1=0.4179\n",
      "Epoch 230, Loss: 0.0287, Weights: w_2=0.2144, w_1=0.4238\n",
      "Epoch 240, Loss: 0.0247, Weights: w_2=0.2133, w_1=0.4293\n",
      "Epoch 250, Loss: 0.0213, Weights: w_2=0.2124, w_1=0.4344\n",
      "Epoch 260, Loss: 0.0183, Weights: w_2=0.2115, w_1=0.4391\n",
      "Epoch 270, Loss: 0.0158, Weights: w_2=0.2107, w_1=0.4435\n",
      "Epoch 280, Loss: 0.0136, Weights: w_2=0.2099, w_1=0.4476\n",
      "Epoch 290, Loss: 0.0117, Weights: w_2=0.2092, w_1=0.4514\n",
      "Epoch 300, Loss: 0.0101, Weights: w_2=0.2085, w_1=0.4549\n",
      "Epoch 310, Loss: 0.0087, Weights: w_2=0.2079, w_1=0.4581\n",
      "Epoch 320, Loss: 0.0075, Weights: w_2=0.2073, w_1=0.4612\n",
      "Epoch 330, Loss: 0.0064, Weights: w_2=0.2068, w_1=0.4640\n",
      "Epoch 340, Loss: 0.0055, Weights: w_2=0.2063, w_1=0.4666\n",
      "Epoch 350, Loss: 0.0048, Weights: w_2=0.2059, w_1=0.4690\n",
      "Epoch 360, Loss: 0.0041, Weights: w_2=0.2054, w_1=0.4712\n",
      "Epoch 370, Loss: 0.0035, Weights: w_2=0.2050, w_1=0.4733\n",
      "Epoch 380, Loss: 0.0030, Weights: w_2=0.2047, w_1=0.4752\n",
      "Epoch 390, Loss: 0.0026, Weights: w_2=0.2043, w_1=0.4770\n",
      "Epoch 400, Loss: 0.0023, Weights: w_2=0.2040, w_1=0.4787\n",
      "Epoch 410, Loss: 0.0019, Weights: w_2=0.2037, w_1=0.4802\n",
      "Epoch 420, Loss: 0.0017, Weights: w_2=0.2035, w_1=0.4816\n",
      "Epoch 430, Loss: 0.0014, Weights: w_2=0.2032, w_1=0.4830\n",
      "Epoch 440, Loss: 0.0012, Weights: w_2=0.2030, w_1=0.4842\n",
      "Epoch 450, Loss: 0.0011, Weights: w_2=0.2028, w_1=0.4853\n",
      "Epoch 460, Loss: 0.0009, Weights: w_2=0.2026, w_1=0.4864\n",
      "Epoch 470, Loss: 0.0008, Weights: w_2=0.2024, w_1=0.4874\n",
      "Epoch 480, Loss: 0.0007, Weights: w_2=0.2022, w_1=0.4883\n",
      "Epoch 490, Loss: 0.0006, Weights: w_2=0.2021, w_1=0.4891\n",
      "Epoch 500, Loss: 0.0005, Weights: w_2=0.2019, w_1=0.4899\n",
      "Epoch 510, Loss: 0.0004, Weights: w_2=0.2018, w_1=0.4906\n",
      "Epoch 520, Loss: 0.0004, Weights: w_2=0.2016, w_1=0.4913\n",
      "Epoch 530, Loss: 0.0003, Weights: w_2=0.2015, w_1=0.4919\n",
      "Epoch 540, Loss: 0.0003, Weights: w_2=0.2014, w_1=0.4925\n",
      "Epoch 550, Loss: 0.0002, Weights: w_2=0.2013, w_1=0.4931\n",
      "Epoch 560, Loss: 0.0002, Weights: w_2=0.2012, w_1=0.4936\n",
      "Epoch 570, Loss: 0.0002, Weights: w_2=0.2011, w_1=0.4940\n",
      "Epoch 580, Loss: 0.0002, Weights: w_2=0.2010, w_1=0.4945\n",
      "Epoch 590, Loss: 0.0001, Weights: w_2=0.2010, w_1=0.4949\n",
      "Epoch 600, Loss: 0.0001, Weights: w_2=0.2009, w_1=0.4952\n",
      "Epoch 610, Loss: 0.0001, Weights: w_2=0.2008, w_1=0.4956\n",
      "Epoch 620, Loss: 0.0001, Weights: w_2=0.2008, w_1=0.4959\n",
      "Epoch 630, Loss: 0.0001, Weights: w_2=0.2007, w_1=0.4962\n",
      "Epoch 640, Loss: 0.0001, Weights: w_2=0.2007, w_1=0.4965\n",
      "Epoch 650, Loss: 0.0001, Weights: w_2=0.2006, w_1=0.4967\n",
      "Epoch 660, Loss: 0.0000, Weights: w_2=0.2006, w_1=0.4970\n",
      "Epoch 670, Loss: 0.0000, Weights: w_2=0.2005, w_1=0.4972\n",
      "Epoch 680, Loss: 0.0000, Weights: w_2=0.2005, w_1=0.4974\n",
      "Epoch 690, Loss: 0.0000, Weights: w_2=0.2005, w_1=0.4976\n",
      "Epoch 700, Loss: 0.0000, Weights: w_2=0.2004, w_1=0.4977\n",
      "Epoch 710, Loss: 0.0000, Weights: w_2=0.2004, w_1=0.4979\n",
      "Epoch 720, Loss: 0.0000, Weights: w_2=0.2004, w_1=0.4981\n",
      "Epoch 730, Loss: 0.0000, Weights: w_2=0.2003, w_1=0.4982\n",
      "Epoch 740, Loss: 0.0000, Weights: w_2=0.2003, w_1=0.4983\n",
      "Epoch 750, Loss: 0.0000, Weights: w_2=0.2003, w_1=0.4984\n",
      "Epoch 760, Loss: 0.0000, Weights: w_2=0.2003, w_1=0.4986\n",
      "Epoch 770, Loss: 0.0000, Weights: w_2=0.2003, w_1=0.4987\n",
      "Epoch 780, Loss: 0.0000, Weights: w_2=0.2002, w_1=0.4988\n",
      "Epoch 790, Loss: 0.0000, Weights: w_2=0.2002, w_1=0.4988\n",
      "Epoch 800, Loss: 0.0000, Weights: w_2=0.2002, w_1=0.4989\n",
      "Epoch 810, Loss: 0.0000, Weights: w_2=0.2002, w_1=0.4990\n",
      "Epoch 820, Loss: 0.0000, Weights: w_2=0.2002, w_1=0.4991\n",
      "Epoch 830, Loss: 0.0000, Weights: w_2=0.2002, w_1=0.4991\n",
      "Epoch 840, Loss: 0.0000, Weights: w_2=0.2001, w_1=0.4992\n",
      "Epoch 850, Loss: 0.0000, Weights: w_2=0.2001, w_1=0.4993\n",
      "Epoch 860, Loss: 0.0000, Weights: w_2=0.2001, w_1=0.4993\n",
      "Epoch 870, Loss: 0.0000, Weights: w_2=0.2001, w_1=0.4994\n",
      "Epoch 880, Loss: 0.0000, Weights: w_2=0.2001, w_1=0.4994\n",
      "Epoch 890, Loss: 0.0000, Weights: w_2=0.2001, w_1=0.4995\n",
      "Epoch 900, Loss: 0.0000, Weights: w_2=0.2001, w_1=0.4995\n",
      "Epoch 910, Loss: 0.0000, Weights: w_2=0.2001, w_1=0.4995\n",
      "Epoch 920, Loss: 0.0000, Weights: w_2=0.2001, w_1=0.4996\n",
      "Epoch 930, Loss: 0.0000, Weights: w_2=0.2001, w_1=0.4996\n",
      "Epoch 940, Loss: 0.0000, Weights: w_2=0.2001, w_1=0.4996\n",
      "Epoch 950, Loss: 0.0000, Weights: w_2=0.2001, w_1=0.4997\n",
      "Epoch 960, Loss: 0.0000, Weights: w_2=0.2001, w_1=0.4997\n",
      "Epoch 970, Loss: 0.0000, Weights: w_2=0.2001, w_1=0.4997\n",
      "Epoch 980, Loss: 0.0000, Weights: w_2=0.2001, w_1=0.4997\n",
      "Epoch 990, Loss: 0.0000, Weights: w_2=0.2000, w_1=0.4997\n",
      "Final Weights: w_2=0.2000, w_1=0.4998 Elapsed Time: 9.97 ms\n",
      "Predictions: x = 6.0 -> 14.200190243275703\n",
      "Real Data: x = 6.0 -> 14.200000000000001\n"
     ]
    }
   ],
   "source": [
    "#Exercise 3-2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# -------- CONFIG --------\n",
    "w_2 = 0.0\n",
    "w_1 = 0.0\n",
    "\n",
    "x_data = np.arange(1.0, 5.1, 1)\n",
    "y_data = [(0.2 * x ** 2) + (0.5 * x) + 4.0 for x in x_data]\n",
    "\n",
    "# -------- HELPER --------\n",
    "def foward(x):\n",
    "    return (w_2 * (x ** 2)) + (w_1 * x) + 4.0\n",
    "\n",
    "def loss(x, y):\n",
    "    y_pred = foward(x)\n",
    "    return (y_pred - y) ** 2\n",
    "\n",
    "def gradient(x, y):\n",
    "    return (2 * (x**2)) * (foward(x) - y), 2 * x * (foward(x) - y)     # (w_2, w_1)\n",
    "\n",
    "# -------- TRAIN LOOP --------\n",
    "train_time = time.time()\n",
    "for epoch in range(1000):\n",
    "    for x, y in zip(x_data, y_data):\n",
    "        grad_2, grad_1 = gradient(x, y)\n",
    "        w_2 -= 0.001 * grad_2\n",
    "        w_1 -= 0.001 * grad_1 \n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        total_loss = sum(loss(x, y) for x, y in zip(x_data, y_data))\n",
    "        print(f'Epoch {epoch}, Loss: {total_loss:.4f}, Weights: w_2={w_2:.4f}, w_1={w_1:.4f}')\n",
    "\n",
    "finish_time = time.time()\n",
    "# Final output\n",
    "print(f'Final Weights: w_2={w_2:.4f}, w_1={w_1:.4f} Elapsed Time: {(finish_time - train_time) * 1000.0:.2f} ms')\n",
    "print(f\"Predictions: x = 6.0 -> {foward(6.0)}\")\n",
    "print(f\"Real Data: x = 6.0 -> {0.2 * 6.0 * 6.0 + 0.5 * 6.0 + 4.0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e97b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
